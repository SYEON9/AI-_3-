{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.Word2Vec.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMa7OVE+XaShM3swYMESSqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SYEON9/natural_language_3th/blob/main/NLP/02_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vcec\n",
        "\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만든다.\n",
        "2. CBOW, Skip-gram 모델을 각각 구현한다.\n",
        "3. 모델을 실제로 학습하고 결과를 확인한다. \n",
        "4. 산점도를 그려 단어들의 대략적인 위치를 확인한다. \n",
        "\n"
      ],
      "metadata": {
        "id": "ziTzw1jAZmrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 Word2Vec에 대해 간단하게 알아보자.\n",
        "\n",
        "One-hot vector는 단어 벡터 간 유의미한 유사도를 계산할 수 없다. 그래서 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법이 필요하다. 이를 위해 사용되는 대표적인 방법이 Word2Vec이다. \n",
        "\n",
        "word2vec을 실행하기 위해서는 우선 수치화된 단어가 필요하다. 그럼 단어를 수치화 하는 것을 뭐라고 할까?\n",
        "\n",
        "다차원 공간에 단어의 의미를 벡터화하는 방법을 분산 표현이라고 하고 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화 하는 작업 == word embedding. 이것으로 표현된 벡터== embedding vector.\n",
        "\n",
        "분산 표현은 저차원에 단어의 의미를 여러 차원에 분산하여 표현하므로 단어 벡터 간 유의미한 유사도를 계샇날 수 있다. \n",
        "\n",
        "word2vec에서는 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다. 즉, 각 단어 벡터가 유사한 벡터값을 가진다는 의미이다.  \n",
        "___"
      ],
      "metadata": {
        "id": "W_xf9xEYdpLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW vs Skip-gram\n",
        "\n",
        "Word2Vec의 학습 방식에는 CBOW, Skip-gram 두가지가 있다. 두 방법의 메커니즘 자체는 비슷하다. \n",
        "\n",
        "- CBOW: 주변 단어를 입력으로 중간 단어를 예측.\n",
        "- Skip-Gram: 중간 단어를 입력으로 주변 단어를 예측.\n",
        "\n"
      ],
      "metadata": {
        "id": "ike-xkoqfqiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 필요패키지 import"
      ],
      "metadata": {
        "id": "cT5soovmZ3P9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llMsGpVLYB_Q"
      },
      "outputs": [],
      "source": [
        "#colab에서 nanum font 사용하기.\n",
        "#폰트 설치\n",
        "!sudo apt-get install -y fonts-nanum    #nanum font install\n",
        "!sudo fc-cache -fv                      #ubuntu font install\n",
        "!rm ~/. cache/matplotlib -rf            #matplotlib이 사용할 font 정보를 삭제..?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 처리 패키지(konlpy) 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "7jub7z-BaHUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from konlpy.tag import Mecab, Twitter, Okt, Kkma\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LJTZC15OboAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ax_detPocKXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2YFCe2CmcLdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 전처리\n",
        "\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리한다. "
      ],
      "metadata": {
        "id": "T5QI2FMxcLqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [\n",
        "  \"정말 맛있습니다. 추천합니다.\",\n",
        "  \"기대했던 것보단 별로였네요.\",\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"\n",
        "]\n",
        "\n",
        "test_words = ['음식','맛','서비스','위생','가격']"
      ],
      "metadata": {
        "id": "eHpY54GvcSrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사하다."
      ],
      "metadata": {
        "id": "CGiPcdYwjXnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Okt()"
      ],
      "metadata": {
        "id": "VqOJr6VBjWZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화하는 함수 make_tokenized를 만들자.\n",
        "def make_tokenized(data):\n",
        "    tokenized = []\n",
        "    for sent in tqdm(data):\n",
        "        #텍스트를 형태소 단위로 나눈다. stem을 사용해 어간을 추출한다.\n",
        "        tokens = tokenizer.morphs(sent, stem = True)     \n",
        "        tokenized.append(tokens)\n",
        "    \n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "QzUuLzZGjfYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장을 토큰화하자.\n",
        "train_tokenized = make_tokenized(train_data)"
      ],
      "metadata": {
        "id": "_aJj963pj7h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized"
      ],
      "metadata": {
        "id": "NYNEKZHLRckT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 토큰들이 각각 몇개씩 존재하는지 세자.\n",
        "word_count = defaultdict(int)    #int값이 default인 딕셔너리 생성.\n",
        "\n",
        "for tokens in tqdm(train_tokenized):\n",
        "    for token in tokens:\n",
        "        word_count[token] += 1\n"
      ],
      "metadata": {
        "id": "hoT20JTZk5U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(word_count))"
      ],
      "metadata": {
        "id": "bCh7cDYtRsC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 등장횟수가 많은 순서대로 정렬하여 확인하자. \n",
        "word_count = sorted(word_count.items(), key=lambda x:x[1], reverse = True)\n",
        "print(list(word_count))"
      ],
      "metadata": {
        "id": "Az5QTd6tlvtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#만들어진 token을 이용하여 각 token에 index를 부여하자. \n",
        "\n",
        "w2i = {}\n",
        "for pair in tqdm(word_count):\n",
        "    if pair[0] not in w2i:\n",
        "        w2i[pair[0]] = len(w2i)\n",
        "\n",
        "i2w = {v:k for k,v in w2i.items()}"
      ],
      "metadata": {
        "id": "eW-fVZx5Tt5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokenized)\n",
        "print(w2i)"
      ],
      "metadata": {
        "id": "UD-f99glUG5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW\n",
        "\n",
        "CBOW는 주변단어를 이용해 주어진 단어(=중심단어)를 예측하는 방법이다. 그러므로 몇개의 주변단어를 참고할지 정하는 window에 따라 input의 개수가 달라진다. 출력은 중심단어 하나이다. \n",
        "\n",
        "\n",
        "참고자료\n",
        "\n",
        "* https://simonezz.tistory.com/35 \n",
        "* https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314 "
      ],
      "metadata": {
        "id": "_Qq4Q0eVmIOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제 모델에 들어가기위한 input을 만들기위해 Dataset 클래스를 정의한다. "
      ],
      "metadata": {
        "id": "xM0JcyI8O4Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, train_tokenized, window_size=2):\n",
        "        self.x = []   #input word\n",
        "        self.y = []   #target word\n",
        "\n",
        "        \n",
        "        for tokens in tqdm(train_tokenized):\n",
        "            #token_ids: token의 id를 저장.\n",
        "            token_ids = [w2i[token] for token in tokens]\n",
        "            print(token_ids) \n",
        "\n",
        "            for i, id in enumerate(token_ids):\n",
        "                if i-window_size >= 0 and i+window_size <len(token_ids):\n",
        "                    #찾으려는 i를 제외하고 input에 필요한 데이터 범위 설정. \n",
        "                    self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "                    self.y.append(id)\n",
        "\n",
        "            self.x = torch.LongTensor(self.x)     #(전체 데이터 개수, 2*window_size)\n",
        "            self.y = torch.LongTensor(self.y)     #(전체 데이터 개수)\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.x.shape[1]\n",
        "\n",
        "    def __gititem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "79FfE4yWmvPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = [] # input word\n",
        "    self.y = [] # target word\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      #token_ids: token의 id를 저장\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      print(token_ids)\n",
        "\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "            #찾으려는 i를 제외하고 input에 필요한 데이터 범위 설정\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.y.append(id)\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "JcXXGdfDW7nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW의 Dataset 객체 생성.\n",
        "cbow_set = CBOWDataset(train_tokenized)\n",
        "print(list(cbow_set))"
      ],
      "metadata": {
        "id": "jBh4gD8Wmkw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dQwRvpr_XUow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####모델 Class 구현\n",
        "\n",
        "CBOW Word2Vec 모델을 구현한다.\n",
        "* `self.embedding`: `vocab_size` 크기의 one-hot vector를 트적 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "* `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer."
      ],
      "metadata": {
        "id": "0aRZACHLXhyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x):  # x: (B, 2W)\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "metadata": {
        "id": "wylVmJWDXg6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW 모델을 생성한다. "
      ],
      "metadata": {
        "id": "2knUx-4JaLKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cbow = CBOW(vocab_size = len(w2i), dim = 256)"
      ],
      "metadata": {
        "id": "tIIdVRbBaNJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kUPMR7WKaI5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델 학습\n",
        "\n",
        "다음과 같이 hyperparameter를 세팅하고 `DataLoader` 객체를 만든다. "
      ],
      "metadata": {
        "id": "u9faMWjIaTw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "uqVTBxjtaaEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW 모델 학습"
      ],
      "metadata": {
        "id": "9WoLcAhwaxdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cbow.train()\n",
        "cbow = cbow.to(device)\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  \n",
        "  for batch in tqdm(cbow_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = cbow(x)  # (B, V)\n",
        " \n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "metadata": {
        "id": "fDwRQ4wZataE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lh4TAXPEbtg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-gram\n",
        "\n",
        "중심 단어를 이용하여 주변 단어를 예측하는 방법이다. 데이터셋을 구성할 때, input x와 target y를 어떻게 설정하는지 CBOW와 비교하면서 살펴보자. "
      ],
      "metadata": {
        "id": "IGbLaQ7Lca-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input을 만들기 위해 Dataset 클래스를 정의한다. "
      ],
      "metadata": {
        "id": "wRj5tZCFcraU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.x += [id] * 2 * window_size\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "POkCCXuUcvhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 생성"
      ],
      "metadata": {
        "id": "8kM7tM_as_tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_set = SkipGramDataset(train_tokenized)\n",
        "print(list(skipgram_set))"
      ],
      "metadata": {
        "id": "wGd3EZ8QsE0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class 모델 구현하기\n",
        "\n",
        "`self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\n"
      ],
      "metadata": {
        "id": "z3o6-szetNKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "    \n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x): # x: (B)\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "metadata": {
        "id": "XjGrJGbmtIbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 생성"
      ],
      "metadata": {
        "id": "yHRtI_8XuI_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram = SkipGram(vocab_size = len(w2i), dim = 256)"
      ],
      "metadata": {
        "id": "xQpdGl9st_Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델 학습\n",
        "\n",
        "hyperparameter를 세팅하고 DataLoader 객체를 만든다. "
      ],
      "metadata": {
        "id": "-57j9McsuRnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "IwQ8SBjfuQdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음은 SkipGram 모델 학습이다."
      ],
      "metadata": {
        "id": "twax83thuine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram.train()\n",
        "skipgram = skipgram.to(device)\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr = learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "    print(\"#\"*50)\n",
        "    print(f\"Epoch:{e}\")\n",
        "\n",
        "    for batch in tqdm(skipgram_loader):\n",
        "        x, y = batch    #각각 정보를 넣음.\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = skipgram(x)\n",
        "\n",
        "        # train\n",
        "        optim.zero_grad()\n",
        "        loss = loss_function(output, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    print(f\"Train loss:{loss.item()}\")\n",
        "print(\"Finished.\")\n"
      ],
      "metadata": {
        "id": "6NPunKt1uhih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6uuzUVyNwrtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DigCzWOuw0du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 테스트\n",
        "\n",
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인하자."
      ],
      "metadata": {
        "id": "hBXQxUtew0nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CBOW\n",
        "\n",
        "for word in test_words:\n",
        "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "    emb = cbow.embedding(input_id)\n",
        "\n",
        "    print(f\"Word: {word}\")\n",
        "    print(emb.squeeze(0))"
      ],
      "metadata": {
        "id": "e7iKM78jw6fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SkipGram\n",
        "\n",
        "for word in test_words:\n",
        "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "    emb = skipgram.embedding(input_id)\n",
        "\n",
        "    print(f\"Word: {word}\")\n",
        "    print(max(emb.squeeze(0)))"
      ],
      "metadata": {
        "id": "38SYSdO5xLkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_words"
      ],
      "metadata": {
        "id": "szbZwZvbxmWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i2w[35]"
      ],
      "metadata": {
        "id": "gxGoUQ_PxsH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity\n",
        "\n",
        "def most_similar(word, top_k = 5):\n",
        "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "    input_emb = skipgram.embedding(input_id)\n",
        "    score = torch.matmul(input_emb, skipgram.embedding.weight.transpose(1,0)).view(-1)\n",
        "\n",
        "    _, top_k_ids = torch.topk(score, top_k)    #주어진 결과 중에서 가장 확률이 높은 값을 top_k개 반환\n",
        "\n",
        "    return [i2w[word_id.item()] for word_id in top_k_ids][1:]"
      ],
      "metadata": {
        "id": "JiE0VWCbxtnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('가격')"
      ],
      "metadata": {
        "id": "9Bhign8pyZfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jrAOOAHXybsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N2GnftW7yxcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec 시각화"
      ],
      "metadata": {
        "id": "o5uh2Jfsyxkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#matplotlib의 한글 깨짐 처리 시작\n",
        "plt.rc('font', family='NanumBarunGothic')\n"
      ],
      "metadata": {
        "id": "GhYjQlgDy0Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)"
      ],
      "metadata": {
        "id": "O6QGezF7zCfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc_weight = pca.fit_transform(skipgram.embedding.weight.data.cpu().numpy())"
      ],
      "metadata": {
        "id": "SYLes3MQzFtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,15))\n",
        "\n",
        "for word_id,(x_coordinate,y_coordinate) in enumerate(pc_weight):\n",
        "  plt.scatter(x_coordinate,y_coordinate,color=\"blue\")\n",
        "  plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
      ],
      "metadata": {
        "id": "3D8RFZDJzNsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZNZiKsdrzi95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}