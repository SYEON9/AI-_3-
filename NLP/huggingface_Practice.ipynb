{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huggingface Practice.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPRipLx/azlAlmeOXt0fmBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SYEON9/natural_language_3th/blob/main/NLP/huggingface_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSqXRMuEsGNL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "Bo2-uBXdsKIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface's Transformers\n",
        "\n",
        "huggingface는 pytorch version의 BERT를 가장 먼저 구현하여 주목을 받았다. 현재는 transformer 기반의 다양한 모델을 구현 및 공개하고 있다. "
      ],
      "metadata": {
        "id": "GqU18e-7soI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main classes\n",
        "\n",
        "AutoConfig에서는 다양한 모델의 configuration(환경설정)을 string tag을 이용해 쉽게 load할 수 있다. 각 config에는 해당 모델의 architecture, task에 대한 정보를 담고 있다.(architecture 종류, 레이어 수, hidden unit size, hyperparameter)"
      ],
      "metadata": {
        "id": "mumsRpzZtNJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
        "config"
      ],
      "metadata": {
        "id": "hmEG3df5sZHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_config = AutoConfig.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "NpRSTxeHtzOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_config"
      ],
      "metadata": {
        "id": "miy0LWuft5ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(config.vocab_size)"
      ],
      "metadata": {
        "id": "Ut9N9-Pgt7Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = config.to_dict()\n",
        "config_dict"
      ],
      "metadata": {
        "id": "a8hABjUBt-Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig\n",
        "\n",
        "# bert type의 사전 학습된 config 정보 불러오기.\n",
        "bertconfig = BertConfig.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "QVC0EgznuCcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_in_gpt2_config = BertConfig.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "663_YkkduQhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oI92RxF5u9dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM, BertForQuestionAnswering, BertForSequenceClassification, BertForTokenClassification, BertForMultipleChoice, BertModel "
      ],
      "metadata": {
        "id": "5PgSSJ6ZuzVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoConfig"
      ],
      "metadata": {
        "id": "woSiqV5VvVue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "0yJnvBspvaoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "input = tokenizer('hi, my name is Taehee')"
      ],
      "metadata": {
        "id": "x_HFvfrnvgMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "id": "oicw0h3uvxlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "oxL-WBoSvz_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa"
      ],
      "metadata": {
        "id": "otm6q2f8wAY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa.state_dict()"
      ],
      "metadata": {
        "id": "sXJvfG6pwIPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa = AutoModel.from_pretrained('deepset/bert-base-cased-squad2')"
      ],
      "metadata": {
        "id": "ISwcm_CTwLdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_token_cls = BertForTokenClassification.from_pretrained('ckiplab/bert-base-chinese-ner')"
      ],
      "metadata": {
        "id": "t3iy3hXkwTQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I-SAAUbCxEzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimization은 널리 쓰이고 있는 다양한 optimizer를 제공한다.\n",
        "이와 관련하여 learning rate를 조절하는 scheduler도 제공한다. "
      ],
      "metadata": {
        "id": "1HgxW1GI0456"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "JESg6PN116Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_maskedlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "parameters = bert_maskedlm.parameters()\n",
        "# parameters = bert_maskedlm.named_parameters()\n",
        "optimizer = AdamW(parameters, lr=5e-5)\n",
        "total_training_step = 100\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_training_step/10), num_training_steps=total_training_step)\n",
        "\n",
        "# loss.backward()\n",
        "optimizer.step()\n",
        "scheduler.step()"
      ],
      "metadata": {
        "id": "wHc_3BDT1H9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainig Movie Review Classifier with BERTForSequenceClassification Class\n",
        "\n",
        "pre-trained BERT의 config, tokenizer, model을 각각 불러온다. "
      ],
      "metadata": {
        "id": "bDzkA6zA2_LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "ypVaJpyK12Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "1Eg66BoR3VXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 불러온다."
      ],
      "metadata": {
        "id": "ZCulYf203vk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset('imdb')"
      ],
      "metadata": {
        "id": "Hv6Td4mA3l8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "Hhm_Xfpg4RQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print(datasets.list_datasets())"
      ],
      "metadata": {
        "id": "sgxkIfGk33jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length = 512"
      ],
      "metadata": {
        "id": "8SoQzkdO3_ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example['text'], padding = 'max_length', truncation = True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched = True)"
      ],
      "metadata": {
        "id": "Wsw7Qowo4fUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets['test'].shuffle(seed = 42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets['train']\n",
        "full_eval_dataset = tokenized_datasets['test']"
      ],
      "metadata": {
        "id": "YYc2UXod4xRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WbXtvhgm5WAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 1: Transformers library를 이용한 영화 리뷰 분류기 학습"
      ],
      "metadata": {
        "id": "_TfMpRSs5XFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "ino_1p0M5b4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"test_trainer\")\n",
        "# 전체 dataset 학습/평가을 원하시는 분들은 full_train_dataset, full_eval_dataset을 사용하시면 됩니다.\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)"
      ],
      "metadata": {
        "id": "EN-zbc1D5kXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "3yTfu9Ay55nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "model = BertForSequenceClassification.from_pretrained('finiteautomata/beto-sentiment-analysis')\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)"
      ],
      "metadata": {
        "id": "yXU60hUh6FgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}