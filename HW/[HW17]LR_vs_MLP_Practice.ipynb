{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HW17]LR vs MLP Practice.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNtVlw9E4ssUL4K4CYo7Flq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SYEON9/natural_language_3th/blob/main/HW/%5BHW17%5DLR_vs_MLP_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGZqIa5UR7Pd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LR vs MLP\n",
        "\n",
        "이전 시간까지는 layer를 하나만 쌓아서 학습했다. 이번에는 layer 개수에 따라 성능이 어떻게 바뀌는지에 대해서 알아보자. \n",
        "\n",
        "layer를 하나 쌓아서 학습하는 linear regression과 non-linear를 추가해서 layer 2개를 쌍항서 만든 MLP를 비교해보자. \n",
        "\n",
        "데이터는 metal-casting parts dataset을 활용한다. "
      ],
      "metadata": {
        "id": "IVlLSl2ASD1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9yB4Pb9GSYGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LR vs MLP\n",
        "이전 시간까지는 layer를 하나만 쌓아서 학습을 했다. 이번 시간에는 layer의 개수에 따라서 성능이 어떻게 바뀌는지에 대해서 알아보도록 하자.\n",
        "\n",
        "Layer를 하나 쌓아서 학습하는 Linear regression과 non-linear 를 추가해서 layer 2개를 쌓아서 만든 MLP를 비교해보자. \n",
        "\n",
        "데이터는 metal-casting parts dataset을 활용한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "msvtY3VdSbJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rtsdf2XqSZpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#images of metal-casting parts"
      ],
      "metadata": {
        "id": "CxEDGTLISr-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autograd import numpy\n",
        "from autograd import grad\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "UiuH1kx7W-lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "URL = 'https://github.com/engineersCode/EngComp6_deeplearning/raw/master/data/casting_images.npz'\n",
        "urlretrieve(URL, 'casting_images.npz')"
      ],
      "metadata": {
        "id": "FEJwbEGQXEHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read in images and labels\n",
        "with numpy.load('/content/casting_images.npz', allow_pickle = True) as data:\n",
        "    ok_images = data['ok_images']\n",
        "    def_images = data['def_images']"
      ],
      "metadata": {
        "id": "cc4xth9FXVr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(ok_images)"
      ],
      "metadata": {
        "id": "EA_OLGMlXkYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ok_images.dtype"
      ],
      "metadata": {
        "id": "MQ1xGd0FXnni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ok_images.shape"
      ],
      "metadata": {
        "id": "K7NEThwBXpkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def_images.shape"
      ],
      "metadata": {
        "id": "SkyDiJs_XsFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HgJSzAXtXuor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[HW14]에서 했던 것과 동일하게 진행한다."
      ],
      "metadata": {
        "id": "2odXummvXwwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_ok_total = ok_images.shape[0]\n",
        "res = int(numpy.sqrt(def_images.shape[1]))   #이미지의 가로/세로 크기 찾기\n",
        "\n",
        "print(\"Number of images without defects: \", n_ok_total)\n",
        "print(\"Image resolution: {} by {}\".format(res, res))"
      ],
      "metadata": {
        "id": "FQcjgorgXzgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_def_total = def_images.shape[0]\n",
        "print(\"Number of images with defects: \", n_def_total)"
      ],
      "metadata": {
        "id": "1NfadK12YMna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = pyplot.subplots(2,3, figsize = (8,6), tight_layout = True)\n",
        "axes[0,0].imshow(ok_images[0].reshape((res,res)), cmap = 'gray')\n",
        "axes[0,1].imshow(ok_images[50].reshape((res,res)), cmap = 'gray')\n",
        "axes[0, 2].imshow(ok_images[100].reshape((res, res)), cmap=\"gray\")\n",
        "axes[1, 0].imshow(ok_images[150].reshape((res, res)), cmap=\"gray\")\n",
        "axes[1, 1].imshow(ok_images[200].reshape((res, res)), cmap=\"gray\")\n",
        "axes[1, 2].imshow(ok_images[250].reshape((res, res)), cmap=\"gray\")\n",
        "fig.suptitle('Casting parts without defects', fontsize = 20);"
      ],
      "metadata": {
        "id": "DYmWrqObYV4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = pyplot.subplots(2,3,figsize = (8,6), tight_layout = True)\n",
        "axes[0,0].imshow(def_images[0].reshape((res,res)), cmap = 'gray')\n",
        "axes[0, 1].imshow(def_images[50].reshape((res, res)), cmap=\"gray\")\n",
        "axes[0, 2].imshow(def_images[100].reshape((res, res)), cmap=\"gray\")\n",
        "axes[1, 0].imshow(def_images[150].reshape((res, res)), cmap=\"gray\")\n",
        "axes[1, 1].imshow(def_images[200].reshape((res, res)), cmap=\"gray\")\n",
        "axes[1, 2].imshow(def_images[250].reshape((res, res)), cmap=\"gray\")\n",
        "fig.suptitle(\"Casting parts with defects\", fontsize = 20);"
      ],
      "metadata": {
        "id": "nGgBq3uebZtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WrtCYUx9biB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split Dataset\n",
        "\n",
        "학습과 평가를 위한 dataset으로 나눈다. train, val, test 데이터에 대한 각각의 개수를 구한다. "
      ],
      "metadata": {
        "id": "XLeu2Encb38D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#number of images for validation(~20%)\n",
        "n_ok_val = int(n_ok_total * 0.2)\n",
        "n_def_val = int(n_def_total * 0.2)\n",
        "print(\"Number of images without defects in validation dataset:\", n_ok_val)\n",
        "print(\"Number of images with defects in validation dataset: \",n_def_val)"
      ],
      "metadata": {
        "id": "-QJl7mN0b7ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of images for test(~20%)\n",
        "n_ok_test = int(n_ok_total * 0.2)\n",
        "n_def_test = int(n_def_total * 0.2)\n",
        "print(\"Number of images without defects in test dataset\", n_ok_test)\n",
        "print(\"Number of images with defects in test dataset\", n_def_test)"
      ],
      "metadata": {
        "id": "IaOHCoitchLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of images for train(~60%)\n",
        "n_ok_train = n_ok_total - n_ok_val - n_ok_test\n",
        "n_def_train = n_def_total - n_def_val - n_def_test\n",
        "print(\"Number of images without defects in train dataset\", n_ok_train)\n",
        "print(\"Number of images with defects in train dataset\", n_def_train)"
      ],
      "metadata": {
        "id": "FufTSA4hchOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g5wjaM_-dQ-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 numpy 패키지 않에 split 함수로 나누어준다."
      ],
      "metadata": {
        "id": "IupvIglzdS7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ok_images = numpy.split(ok_images, [n_ok_val, n_ok_val+n_ok_test], 0)\n",
        "def_images = numpy.split(def_images, [n_def_val, n_def_val+n_def_test], 0)"
      ],
      "metadata": {
        "id": "EEsQTnOFdWhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ok_images"
      ],
      "metadata": {
        "id": "jY8PYRl3fUl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 numpy 패키지 안에 concatenate함수를 이용해서 train, val, test끼리 결함이 있는 이미지와 없는 이미지를 합쳐준다."
      ],
      "metadata": {
        "id": "pUgJ1D-Kf8ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_val = numpy.concatenate([ok_images[0], def_images[0]], 0)\n",
        "images_test = numpy.concatenate([ok_images[1], def_images[1]], 0)\n",
        "images_train = numpy.concatenate([ok_images[2], def_images[2]], 0)"
      ],
      "metadata": {
        "id": "ul5W8PZrgDhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kivpFvB5gSA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mf5BtXtDgzFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data normalization: z-score normalization\n",
        "\n",
        "[HW14]와 동일한 방식으로 진행한다. \n",
        "z = (x-train의 평균) / (train의 표준편차)\n",
        "\n",
        "train, validation, test 모두에 적용한다. "
      ],
      "metadata": {
        "id": "eE4SN953gzRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_train.max(), images_train.min()"
      ],
      "metadata": {
        "id": "h-fVFT0khGNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate mu and sigma\n",
        "mu = numpy.mean(images_train, axis = 0)\n",
        "sigma = numpy.std(images_train, axis = 0)\n",
        "\n",
        "#normalization the training, validation, test dataset\n",
        "images_train = (images_train - mu) / sigma\n",
        "images_val = (images_val - mu) / sigma\n",
        "images_test = (images_test - mu) / sigma"
      ],
      "metadata": {
        "id": "zKMeZto-hK1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_train.max(), images_train.min()"
      ],
      "metadata": {
        "id": "hvtJNor-hnto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JmGIYycrhr31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating labels / classes\n",
        "\n",
        "데이터셋에 class label을 정해주어야 한다. 이 이미지가 결함이 있느지 없는지 명시적으로 나타내주는 것이다. 결함이 있는 것을 1, 없는 것을 0으로 나타내겠다."
      ],
      "metadata": {
        "id": "L2wFgZZ4ht8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#label은 정답으로 원래 데이터에 붙일 필요가 없다. 그러므로 정답 벡터만 만들자!\n",
        "\n",
        "#labels for training data\n",
        "labels_train = numpy.zeros(n_ok_train+n_def_train)\n",
        "labels_train[n_ok_train:] = 1.\n",
        "\n",
        "#labels for validation data\n",
        "labels_val = numpy.zeros(n_ok_val + n_def_val)\n",
        "labels_val[n_ok_val:] = 1.\n",
        "\n",
        "#labels for test data\n",
        "labels_test = numpy.zeros(n_ok_test + n_def_test)\n",
        "labels_test[n_ok_test:] = 1."
      ],
      "metadata": {
        "id": "6Uh3kdWRh9ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test"
      ],
      "metadata": {
        "id": "DLSBYthcjBUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Yef9Z3ajD_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 입력으로 들어온 이미지에 결함이 없는지 알아내기 위해 logistic model을 사용하겠다. 지난번처럼 출력 확률 >= 0.5이면 결함이 있고 출력 확률 < 0.5이면 결함이 없다고 설정한다. "
      ],
      "metadata": {
        "id": "TRtejSJ9jGte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(x, model, params):\n",
        "\n",
        "    probabilites = model(x, params)\n",
        "    labels = (probabilites >= 0.5).astype(float)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "OhVJTJALjY0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r_zGjA-6kD7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating model performance: F-score, Accuracy\n",
        "\n",
        "이제 우리가 학습한 모델이 얼마나 잘 예측하는지 알아보자. \n",
        "\n",
        "우리가 사용할 결과지표는 accuarcy, precision, recall이다. 우리는 precision과 recall로 F-score를 계산할 수 있다. \n"
      ],
      "metadata": {
        "id": "PyPYvWvikFMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def performance(predictions, answers, beta = 1.0):\n",
        "\n",
        "    true_idx = (answers == 1)    #the location where the answers are 1\n",
        "    false_idx = (answers == 0)   #the location where the answers are0\n",
        "\n",
        "    #true positive: answers are 1 and predictions are also 1\n",
        "    n_tp = numpy.count_nonzero(predictions[true_idx] == 1)\n",
        "\n",
        "    #false positive: answers are 0 but predictions are 1\n",
        "    n_fp = numpy.count_nonzero(predictions[false_idx]==1)\n",
        "\n",
        "    #true negative: answers are 0 and predictions are also 0\n",
        "    n_tn = numpy.count_nonzero(predictions[false_idx] == 0)\n",
        "\n",
        "    #false negative: answers are 0 and predictions are 1\n",
        "    n_fn = numpy.count_nonzero(predictions[true_idx]==0)\n",
        "\n",
        "\n",
        "    #precision, recall, and f-score\n",
        "    precision = n_tp/ (n_tp+ n_fp)\n",
        "    recall = n_tp / (n_tp + n_fn)\n",
        "    score = (\n",
        "        (1.0 + beta**2) * precision * recall / \n",
        "        (beta**2 * precision + recall)\n",
        "    )\n",
        "\n",
        "    accuracy = (n_tp + n_tn) / (n_tp + n_fn + n_fp + n_tn)\n",
        "\n",
        "    return precision, recall, score, accuracy\n"
      ],
      "metadata": {
        "id": "k9NRG5bFkefD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YbwsGnF0mUlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model\n",
        "\n",
        "logistic regression과 2 layers MLP model을 구현해보겠다. "
      ],
      "metadata": {
        "id": "D4g6XViJsCkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(x):\n",
        "    return 1. / (1. + numpy.exp(-x))"
      ],
      "metadata": {
        "id": "nuDfsYGSsQNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LR_model(x, params):\n",
        "    \"\"\"A logistic regression model.\n",
        "\n",
        "    A logistic regression is y = sigmoid(x*w + b), where the operator * denotes\n",
        "    a mat-vec multiplication.\n",
        "    \"\"\"\n",
        "\n",
        "    return logistic(numpy.dot(x, params[0])+ params[1])"
      ],
      "metadata": {
        "id": "-iUXASImsV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_model(x, params):\n",
        "    \"\"\"A MLP model.\n",
        "\n",
        "    A MLP is y = sigmoid(max((x*w + b1), 0) *w2 + b2), \n",
        "    where the operator * denotes a mat-vec multiplication.\n",
        "    \"\"\"\n",
        "\n",
        "    x = numpy.dot(x, params[0]) + params[1]\n",
        "    x = numpy.maximum(x, 0)\n",
        "\n",
        "    return logistic(numpy.dot(x, params[2]) + params[3])"
      ],
      "metadata": {
        "id": "6f1bPTPZsxsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m8GaZPm2tgxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 cost function을 만들어보자. logistic regression의 cost function을 사용하겠다. "
      ],
      "metadata": {
        "id": "BJml8KKztioH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_loss(x, true_labels, model, params):\n",
        "    \n",
        "    pred = model(x, params)\n",
        "\n",
        "    loss = -(\n",
        "        numpy.dot(true_labels, numpy.log(pred + 1e-15)) +\n",
        "        numpy.dot(1. - true_labels, numpy.log(1. -pred + 1e-15))\n",
        "    )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Pe5Au4FWtugm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_train.shape[1]"
      ],
      "metadata": {
        "id": "55zYsD1CuDUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initalization\n",
        "\n",
        "LR과 MLP에서 사용할 parameter들을 초기화하자."
      ],
      "metadata": {
        "id": "H_PFnoeIuEXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to get the gradients of a logistic model\n",
        "gradients = grad(model_loss, argnum = 3)     #output의 gradient 합계를 계산하고 반환한다. \n",
        "\n",
        "#initialize LR parameters\n",
        "std = 1e-4\n",
        "LR_w = std * numpy.random.randn(images_train.shape[1])\n",
        "LR_b = numpy.zeros(1)\n",
        "\n",
        "\n",
        "#initalization MLP parameters\n",
        "hidden = 32\n",
        "w0 = std * numpy.random.randn(images_train.shape[1], hidden)\n",
        "b0 = numpy.zeros(hidden)\n",
        "w1 = std * numpy.random.rand(hidden)\n",
        "b1 = numpy.zeros(1)"
      ],
      "metadata": {
        "id": "Er601wwkuJas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pGB1B6Pvvovc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training / optimization\n",
        "\n",
        "이제 두 모델을 학습하고, 성능을 비교해보자.\n",
        "\n",
        "총 5000번 학습을 하는 동안 가장 높은 accuracy를 측정하겠다."
      ],
      "metadata": {
        "id": "FrxVtOaYvtdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d1Gpor7jv2Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#logistic regression"
      ],
      "metadata": {
        "id": "f2ARpM2yv3aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#learning rate\n",
        "lr = 1e-5\n",
        "\n",
        "# a variable for optimization iterations\n",
        "change = numpy.inf\n",
        "\n",
        "#a counter for optimization iterations\n",
        "i = 0\n",
        "\n",
        "# a variable to store the validation loss from the previous iteration\n",
        "old_val_loss = 1e-15\n",
        "best_acc = 0.0\n",
        "\n",
        "#keep running if:\n",
        "#   1.we still see significant changes in validation loss\n",
        "#   2. iteration counter < 10000\n",
        "\n",
        "while i < 5000:\n",
        "\n",
        "    #학습\n",
        "    #calculate gradients and use gradient descents\n",
        "    grads = gradients(images_train, labels_train, LR_model, (LR_w, LR_b))\n",
        "    LR_w -= (grads[0] * lr)\n",
        "    LR_b -= (grads[1] * lr)\n",
        "\n",
        "\n",
        "    #검즘\n",
        "    #validation loss\n",
        "    val_loss = model_loss(images_val, labels_val, LR_model, (LR_w, LR_b))\n",
        "\n",
        "    #calculate f-scores against the validation dataset\n",
        "    pred_labels_val = classify(images_val, LR_model, (LR_w, LR_b))\n",
        "    score = performance(pred_labels_val, labels_val)\n",
        "    best_acc = max(best_acc, score[3])\n",
        "    #calculate the change in validation loss\n",
        "    change = numpy.abs((val_loss - old_val_loss)/old_val_loss)\n",
        "\n",
        "    #update the counter and old_val_loss\n",
        "    i += 1\n",
        "    old_val_loss = val_loss\n",
        "\n",
        "    # print the progress every 10 steps\n",
        "    if i % 10 == 0:\n",
        "        print(\"{}...\".format(i), end = '')\n",
        "    \n",
        "score = performance(pred_labels_val, labels_val)\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"Upon optimization stopped:\")\n",
        "print(\"    Iterations:\", i)\n",
        "print(\"    Best Accuracy:\", best_acc)"
      ],
      "metadata": {
        "id": "fhxGxAjHv6pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PIrHkJ_xyDWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP"
      ],
      "metadata": {
        "id": "Fg41wlh4y-XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#learning rate\n",
        "lr = 1e-15\n",
        "\n",
        "#a variable for the change in validation loss\n",
        "change = numpy.inf\n",
        "\n",
        "# a counter for optimization iterations\n",
        "i = 0\n",
        "\n",
        "# a variables to store the validation loss from the previous iteration\n",
        "old_val_loss = 1e-15\n",
        "best_acc = 0.0\n",
        "\n",
        "\n",
        "#keep running if:\n",
        "#   1. we still see significant changes in validation loss\n",
        "#   2. iteration counter < 10000\n",
        "\n",
        "while i < 5000:\n",
        "\n",
        "    #학습\n",
        "    #calculate gradients and use gradient descents\n",
        "    grads = gradients(images_train, labels_train, MLP_model, (w0, b0, w1, b1))\n",
        "    w0 -= (grads[0] * lr)\n",
        "    b0 -= (grads[1] * lr)\n",
        "    w1 -= (grads[2] * lr)\n",
        "    b1 -= (grads[3] * lr)\n",
        "\n",
        "    \n",
        "    #검증\n",
        "    #validation loss\n",
        "    val_loss = model_loss(images_val, labels_val, MLP_model, (w0, b0, w1, b1))\n",
        "\n",
        "    #calculate f_scores against()\n",
        "    pred_labels_val = classify(images_val, MLP_model, (w0, b0, w1, b1))\n",
        "    score = performance(pred_labels_val, labels_val)\n",
        "    best_acc = max(best_acc, score[3])\n",
        "    # calculate the chage in validation loss\n",
        "    change = numpy.abs((val_loss-old_val_loss)/old_val_loss)\n",
        "\n",
        "    # update the counter and old_val_loss\n",
        "    i += 1\n",
        "    old_val_loss = val_loss\n",
        "    \n",
        "    # print the progress every 10 steps\n",
        "    if i % 10 == 0:\n",
        "        print(\"{}...\".format(i), end=\"\")\n",
        "\n",
        "score = performance(pred_labels_val, labels_val)\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"Upon optimization stopped:\")\n",
        "print(\"    Iterations:\", i)\n",
        "print(\"    Best Accuracy:\", best_acc)"
      ],
      "metadata": {
        "id": "DJ6LKBhnzBZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WR6wsr0G0bo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logisitc regression's accuracy: 84%\n",
        "\n",
        "MLP's accuarcy: 87%\n",
        "\n",
        "결과적으로 MLP의 성능이 더 좋다. layer의 개수가 많아지고 사이에 non-linear function이 추가되면, 더 다양하고 복잡한 함수를 모델링 할 수 있게 된다. \n",
        "\n",
        "그 결과, 데이터를 더 잘 설명할 수 있게 되고, 높은 accuracy를 가질 수 있게 된다. \n",
        "\n"
      ],
      "metadata": {
        "id": "WKvNO8W30dnK"
      }
    }
  ]
}