{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HW13]Polynomial Regression & Regularization practice.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPmjf42jMCjbu3hp1hM7e/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SYEON9/natural_language_3th/blob/main/HW/%5BHW13%5DPolynomial_Regression_%26_Regularization_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF2ua8B0mExU"
      },
      "outputs": [],
      "source": [
        "#linear regression은 데이터를 가장 잘 설명해주는 직선을 찾는 것이었다.\n",
        "#그러나 데이터의 형태가 곡선의 형태라면 어떻게 예측해야할까?\n",
        "#이 문제를 우리는 polymomial regression을 사용해서 알아볼 것이다. \n",
        "#그리고 regularization을 통해 어떻게 학습이 변화되는지 알아보자. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Special case of multiple linear regression"
      ],
      "metadata": {
        "id": "Fab5RYGCpshC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4차 polynomial 함수를 이용하여 데이터를 생성해보자. \n",
        "from matplotlib import pyplot\n",
        "from autograd import grad \n",
        "from autograd import numpy "
      ],
      "metadata": {
        "id": "uZthDjPZpqU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy.random.seed(0)\n",
        "x = numpy.linspace(-3, 3, 20)\n",
        "y = x**4 + x**3 - 4*x**2 + 8*numpy.random.normal(size=len(x))\n",
        "pyplot.scatter(x,y);"
      ],
      "metadata": {
        "id": "DTzXBiEbqII1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터가 직선형태가 아니므로 linear regression 대신 다른 방법을 사용해야 한다. \n",
        "#우리는 polynomial function을 사용하여 데이터를 표현할 것이다. \n",
        "\n",
        "#1. d차 곡선을 정의하자\n",
        "#우리가 찾아야 할 것은 w(가중치)이다.  이 문제를 우리는 행렬의 선형결합 형태로 나타낼 수 있다.\n",
        "#이것을 고려하여 실습해보자.\n",
        "\n",
        "#3차항까지 고려하겠다.\n",
        "degree = 3\n",
        "\n",
        "#행렬의 행태로 x값 변환\n",
        "def polynomial_features(x, degree):\n",
        "    \"\"\" Generate polynomial features for x.\"\"\"\n",
        "    \n",
        "    X = numpy.empty((len(x), degree+1))        #배열 생성. zeros와 달리 메모리를 할당발다 초기화 없이 반환하여 해당 메모리에 남아있는 값이 들어간다. \n",
        "    for i in range(degree+1):\n",
        "        X[:,i] = x**i                          #첫번째 열에는 1을, 나머지 열에는 x의 i제곱을 각각 넣는다. \n",
        "    return X\n",
        "\n",
        "X = polynomial_features(x, degree)\n",
        "print(X.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "kZrY-TwJqfJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "LS86Dpxgt9e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale the data, train the model"
      ],
      "metadata": {
        "id": "_y6_HEzvvMvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input vairable을 normalize하고 학습을 진행하자.\n",
        "#scikit-learn의 min max scaler를 사용한다.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X_scaled = min_max_scaler.fit_transform(X)         #X[:,0]의 값이 0으로 변함.\n",
        "print(X_scaled)\n",
        "X_scaled[:,0] = 1"
      ],
      "metadata": {
        "id": "L1rXIDu2vPQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled.shape"
      ],
      "metadata": {
        "id": "w4KA9j4TvqCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape을 다 다듬었으니 이제 학습을 진행해보자.\n",
        "def linear_regression(params, X):\n",
        "    '''The linear regression model in matrix form. \n",
        "    Arguments:\n",
        "      params: 1D array of weights for the linear model\n",
        "      X     : 2D array of input values\n",
        "    Returns:\n",
        "      1D array of predicted values\n",
        "    '''\n",
        "    return numpy.dot(X, params)                           #numpy.dot(): array를 곱함\n",
        "\n",
        "def mse_loss(params, model, X, y):\n",
        "    '''The mean squared error loss function.\n",
        "    Arguments:\n",
        "      params: 1D array of weights for the linear model\n",
        "      model : function for the linear regression model\n",
        "      X     : 2D array of input values\n",
        "      y     : 1D array of predicted values\n",
        "    Returns:\n",
        "      float, mean squared error\n",
        "    '''\n",
        "    y_pred = model(params, X)\n",
        "    return numpy.mean(numpy.sum((y-y_pred)**2))\n",
        "\n",
        "gradient = grad(mse_loss)"
      ],
      "metadata": {
        "id": "fL-UcLlRvvzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter = 3000\n",
        "alpha = 0.01                               #learning rate\n",
        "params = numpy.zeros(X_scaled.shape[1])\n",
        "descent = numpy.ones(X_scaled.shape[1])\n",
        "i = 0"
      ],
      "metadata": {
        "id": "dTiPGD2uwuvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "while numpy.linalg.norm(descent) > 0.01 and i < max_iter:            #loss의 평균값(표준값)이 0.01보다 작거나 max_iter를 넘기면 멈춘다. \n",
        "    descent  = gradient(params, linear_regression, X_scaled, y)\n",
        "    params = params - descent*alpha\n",
        "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
        "    mae = mean_absolute_error(y, X_scaled@params)\n",
        "    if i%100 == 0:\n",
        "        print(\"iteration {}, loss = {}, mae = {}\".format(i, loss, mae))\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "uVHYl_Mww9ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 params확인\n",
        "params"
      ],
      "metadata": {
        "id": "qiUdGN-pxofQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원래 데이터와 함께 그래프로 그려보자.\n",
        "xgrid = numpy.linspace(x.min(), x.max(), 30)              #linspace: 1차원 벡터 생성.\n",
        "Xgrid_poly_feat = polynomial_features(xgrid, degree)      #데이터 왼쪽에 1벡터를 추가\n",
        "Xgrid_scaled = min_max_scaler.transform(Xgrid_poly_feat)  #추가한 벡터가 0으로 바뀐다.\n",
        "Xgrid_scaled[:,0] = 1                                     #다시 1로 바꿔준다. \n",
        "\n",
        "#시각화\n",
        "pyplot.scatter(x, y, c = 'r', label = 'true')\n",
        "pyplot.plot(xgrid, Xgrid_scaled@params, label = 'predicted')  #xgrid로 범위 지정. \n",
        "pyplot.legend();"
      ],
      "metadata": {
        "id": "wFt88XoBXAuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mLB8bYrUYeM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s-oWwRHOZACi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fLDo6nd1ZAO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jyZvkKriZAVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe Underfitting & Overfitting\n"
      ],
      "metadata": {
        "id": "XUZyAyvCZAxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1~15까지의 다양한 차수를 가지고 학습을 진행해보자. \n",
        "#이 과정에서 어떤 변화가 찾아올까?\n",
        "from urllib.request import urlretrieve\n",
        "URL = 'https://raw.githubusercontent.com/engineersCode/EngComp6_deeplearning/master/scripts/plot_helpers.py'\n",
        "urlretrieve(URL, 'plot_helpers.py')"
      ],
      "metadata": {
        "id": "aLFY9I_8ZFfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#먼저 데이터를 확인하자\n",
        "from plot_helpers import interact_polyreg\n",
        "\n",
        "max_degree = 15\n",
        "interact_polyreg(max_degree, x, y)            #차수에 따른 polynomial regreesion을 살펴보자. "
      ],
      "metadata": {
        "id": "HEs3unI3ZQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "41OlTzGJZe3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P13e1Di0aH9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underfitting"
      ],
      "metadata": {
        "id": "ozK_sbnMaIEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#underfitting하면 우리는 high bias, low variance를 갖게된다. \n",
        "#즉, 예측값을 규칙있게 뽑아내지만(예측값이 한군데에 몰려있지만) 정답과 거리가 있다.\n"
      ],
      "metadata": {
        "id": "kvoKB9W1aKsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting"
      ],
      "metadata": {
        "id": "491qOuIPfOOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#degree를 높게 설정하면 w의 개수가 증가-> 학습 데이터에 대해 잘 예측하고 mae도 감소\n",
        "#그러나 새로운 데이터를 잘 못맞춤. \n",
        "#high variance, low bias이다. \n"
      ],
      "metadata": {
        "id": "tsyt4F1GfSKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization"
      ],
      "metadata": {
        "id": "q9nebzHifu67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#overfitting을 막는 방법. \n",
        "#cost function에 새로운 식을 추가하여 복잡한 모델이 되는 것을 막는다. \n",
        "#regularization을 추가하면 loss를 줄이기 위해 가중치는 작은 값을 선호하게 된다. \n",
        "#여기서 람다는 얼마나 큰 제약을 줄것인가를 결정하고 큰 값을 가질수록 가중치를 더 작게 만든다.\n"
      ],
      "metadata": {
        "id": "5BFrTE95fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#람다를 1로 설정하여 실습을 해보자\n",
        "def regularized_loss(params, model, X, y, _lambda = 1.0):\n",
        "    '''The mean squared error loss function with an L2 penalty.\n",
        "    Arguments:\n",
        "      params: 1D array of weights for the linear model\n",
        "      model : function for the linear regression model\n",
        "      X     : 2D array of input values\n",
        "      y     : 1D array of predicted values\n",
        "      _lambda: regularization parameter, default 1.0\n",
        "    Returns:\n",
        "      float, regularized mean squared error\n",
        "    '''\n",
        "    y_pred = model(params, X)\n",
        "    return numpy.mean( numpy.sum((y-y_pred)**2)) + _lambda * numpy.sum(params[1:]**2)\n",
        "\n",
        "gradient = grad(regularized_loss)"
      ],
      "metadata": {
        "id": "Ym0ZGVLPgM0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_regularization_params = params.copy()"
      ],
      "metadata": {
        "id": "mV4NrLg0hMyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter = 3000\n",
        "alpha = 0.01\n",
        "params = numpy.zeros(X_scaled.shape[1])\n",
        "descent = numpy.ones(X_scaled.shape[1])      #cost의 평균을 담을 것.\n",
        "i = 0\n",
        "\n",
        "#gradient descent\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "while numpy.linalg.norm(descent) > 0.01 and i < max_iter:\n",
        "    descent = gradient(params, linear_regression, X_scaled, y)\n",
        "    params = params - descent*alpha\n",
        "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
        "    mae = mean_absolute_error(y, X_scaled@params)\n",
        "    if i%100 == 0:\n",
        "        print('iteration {}, loss = {}, mae = {}'.format(i, loss, mae))\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "lxmTIxT3hbyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regularization을 추가하지 않은 것과 비교해보자.\n",
        "print('weights without regularization')\n",
        "print(no_regularization_params)\n",
        "print('weights with regularization')\n",
        "print(params)\n",
        "pyplot.scatter(x, y, c = 'r', label = 'true')\n",
        "pyplot.plot(xgrid, Xgrid_scaled@no_regularization_params, label = 'w/0 regularization')\n",
        "pyplot.plot(xgrid, Xgrid_scaled@params, label = 'with regularization')\n",
        "pyplot.legend();"
      ],
      "metadata": {
        "id": "6ogPtUnSifaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#더 다양한 차우에서 regularization을 비교해보자.\n",
        "interact_polyreg(max_degree, x, y, regularized = True)"
      ],
      "metadata": {
        "id": "lVTgcZqYjL2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#degree가 커질수록 regularization의 중요성이 들어난다. \n",
        "#regularization을 사용하면 높은 차수에서 예측 모델이 부드럽게 학습 데이터들을 지나간다. \n",
        "#그러므로 모델의 예측 성능도 개선된다. "
      ],
      "metadata": {
        "id": "SSjcoZY-jn-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}